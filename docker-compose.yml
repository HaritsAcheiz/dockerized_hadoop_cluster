version: '3'

services:
  namenode:
    image: hadoop-spark-base:latest
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"  # Hadoop NameNode web UI
      - "8088:8088"  # YARN ResourceManager web UI
      - "7077:7077"  # Spark Master port
      - "8080:8080"  # Spark Master web UI
      - "2222:22"    # SSH
    volumes:
      - ./hadoop_namenode:/hadoop/dfs/name
      - ./hadoop_config:/opt/hadoop_config
      - ./spark_config:/opt/spark_config
    environment:
      - NODE_TYPE=namenode
      - HDFS_NAMENODE_USER=hdfs
      - YARN_RESOURCEMANAGER_USER=yarn
      - YARN_NODEMANAGER_USER=yarn
    networks:
      hadoop_network:
        aliases:
          - namenode
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9870" ]
      interval: 30s
      timeout: 10s
      retries: 5

  datanode1:
    image: hadoop-spark-base:latest
    container_name: datanode1
    hostname: datanode1
    volumes:
      - ./hadoop_datanode1:/hadoop/dfs/data
      - ./hadoop_config:/opt/hadoop_config
      - ./spark_config:/opt/spark_config
    environment:
      - NODE_TYPE=datanode
      - HDFS_DATANODE_USER=hdfs
    networks:
      hadoop_network:
        aliases:
          - datanode1
    depends_on:
      namenode:
        condition: service_healthy

  datanode2:
    image: hadoop-spark-base:latest
    container_name: datanode2
    hostname: datanode2
    volumes:
      - ./hadoop_datanode2:/hadoop/dfs/data
      - ./hadoop_config:/opt/hadoop_config
      - ./spark_config:/opt/spark_config
    environment:
      - NODE_TYPE=datanode
      - HDFS_DATANODE_USER=hdfs
    networks:
      hadoop_network:
        aliases:
          - datanode2
    depends_on:
      namenode:
        condition: service_healthy

networks:
  hadoop_network:
    name: hadoop_network